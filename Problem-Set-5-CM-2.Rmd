---
title: "Problem Set 5"
author: "Pete Cuppernull"
date: "2/26/2020"
output: 
  pdf_document:
    latex_engine: xelatex
---

##Load Packages and Data
```{r setup, warning=FALSE, message=FALSE}
library(tidyverse)
library(tidymodels)
library(rsample)
library(glmnet)
library(leaps)
library(rcfss)
library(patchwork)
library(caret)
library(h2o)
library(ipred)
library(adabag)
library(randomForest)
library(ranger)
library(gbm)
library(tree)
library(pROC)

gss_test <- na.omit(read_csv("data/gss_test.csv"))
gss_train <- na.omit(read_csv("data/gss_train.csv"))

set.seed(1414)
```
#Conceptual: Cost functions for classification trees

When growing a decision tree, either the GINI index or cross-entropy are preferable to misclassification rate because they have a greater capacity to produce pure nodes.  The best pick between these two approaches is sometimes context dependent, but on average the GINI index would likely be preferable because it is less computationally expnsive than cross-entropy (as it does not require computing logarithms). When pruning a decision tree, typically the misclassification rate is used (see ESL, p. 310).


#Application

##2. Estimate the Models

###Logit
```{r, warning=FALSE, message=FALSE}
gss_train_logit <- as_tibble(gss_train) %>%
  mutate(colrac = factor(colrac))

# function to generate assessment statistics for titanic model
holdout_results <- function(splits) {
  # Fit the model to the training set
  mod <- glm(colrac ~ ., data = analysis(splits),
             family = binomial)
  
  # `augment` will save the predictions with the holdout data set
  res <- augment(mod, newdata = assessment(splits)) %>% 
    as_tibble() %>%
    mutate(.prob = logit2prob(.fitted),
           .pred = round(.prob))

  # Return the assessment data set with the additional columns
  res
}

gss_logit_cv10 <- vfold_cv(data = gss_train_logit, v = 10) %>%
  mutate(results = map(splits, holdout_results)) %>%
  unnest(results) %>%
  mutate(.pred = factor(.pred)) %>%
  group_by(id) %>%
  accuracy(truth = colrac, estimate = .pred)

logit_error <- 1 - mean(gss_logit_cv10$.estimate, na.rm = TRUE)


##roc
logit_roc <- vfold_cv(data = gss_train_logit, v = 10) %>%
  mutate(results = map(splits, holdout_results)) %>%
  unnest(results) %>%
  group_by(id) %>%
  roc(response = colrac, predictor = .pred)


```

###Naive Bayes
```{r, warning=FALSE, message=FALSE, results='hide'}
h2o.no_progress()
h2o.init()

#Save DFs as h20 objects
train.h2o <- as.h2o(gss_train %>%
                      mutate(colrac = as.factor(colrac)))
test.h2o <- as.h2o(gss_test %>%
                      mutate(colrac = as.factor(colrac)))

#set variable names
y <- "colrac"
x <- setdiff(names(gss_train), y) 



#NB Model
nb <- h2o.naiveBayes(
  x = x, 
  y = y,
  training_frame = train.h2o,
  validation_frame = test.h2o,
  seed = 123,
  nfolds = 10)

nb_auc <- h2o.auc(nb, valid = TRUE)

```

NEW
```{r}
gss_train_x <- model.matrix(colrac ~ ., gss_train)[, -1]
gss_train_y <- gss_train$colrac

gss_test_x <- model.matrix(colrac ~ ., gss_test)[, -1]
gss_test_y <- gss_test$colrac

gss_train_caret <- gss_train %>%
  mutate(colrac = make.names(as.character(if_else(colrac == 1, TRUE, FALSE))))

ctrl <- trainControl(method = "cv",     # Cross-validation
                     number = 10,      # 10 folds
                     classProbs = TRUE,                  # For AUC
                     summaryFunction = twoClassSummary)  # For AUC

nb_grid <- expand.grid(
  usekernel = TRUE,
  fL = 1,
  adjust = seq(.5, 2, by = .5)
)


nb_cv <- train(colrac ~ ., 
                       data = gss_train_caret,
                       method = 'nb',
                       metric = 'Accuracy',
                       tuneGrid = nb_grid,
                       trControl = ctrl)


#ROC
nb_cv$results$ROC[which.max(nb_cv$results$ROC)]
#error
nb_error<- ((nb_cv$results$Sens+nb_cv$results$Spec)/2)[which.max(nb_cv$results$Sens+nb_cv$results$Spec)/2]

```





###Elastic net regression
```{r, warning=FALSE, message=FALSE}



lasso    <- glmnet(gss_train_x, gss_train_y, alpha = 1.0) 
elastic1 <- glmnet(gss_train_x, gss_train_y, alpha = 0.25) 
elastic2 <- glmnet(gss_train_x, gss_train_y, alpha = 0.75) 
ridge    <- glmnet(gss_train_x, gss_train_y, alpha = 0.0)

fold_id <- sample(1:10, size = length(gss_train_y), replace = TRUE)

tuning_grid <- tibble::tibble(
  alpha      = seq(0, 1, by = .1),
  mse_min    = NA,
  mse_1se    = NA,
  lambda_min = NA,
  lambda_1se = NA
)

for(i in seq_along(tuning_grid$alpha)) {
  # fit CV model for each alpha value
  fit <- cv.glmnet(gss_train_x, 
                   gss_train_y, 
                   alpha = tuning_grid$alpha[i], 
                   foldid = fold_id)
  
  # extract MSE and lambda values
  tuning_grid$mse_min[i]    <- fit$cvm[fit$lambda == fit$lambda.min]
  tuning_grid$mse_1se[i]    <- fit$cvm[fit$lambda == fit$lambda.1se]
  tuning_grid$lambda_min[i] <- fit$lambda.min
  tuning_grid$lambda_1se[i] <- fit$lambda.1se
}



##now that we have the model, lets generate predictions so we can calculate error rate and AUC
best_elastic_mod <- cv.glmnet(gss_train_x, 
                   gss_train_y, 
                   alpha = 0.3, 
                   foldid = fold_id)

best_elastic_mod_bin <- cv.glmnet(gss_train_x, 
                   gss_train_y, 
                   alpha = 0.3, 
                   foldid = fold_id,
                   family = "binomial")


elastic_error <- mean(best_elastic_mod$cvm)

best_elastic_mod_auc <- cv.glmnet(gss_train_x, 
                   gss_train_y, 
                   alpha = 0.3, 
                   family = "binomial",
                   type.measure = "auc",
                   foldid = fold_id)


elastic_auc <- mean(best_elastic_mod_auc$cvm)
```

###Decision tree (CART)
```{r, warning=FALSE, message=FALSE}
# generate 10-fold CV trees
gss_tree_cv <- gss_train_logit %>%
  vfold_cv(v = 10) %>%
  mutate(tree = map(splits, ~ tree(colrac ~ ., data = analysis(.x),
                                   control = tree.control(nobs = nrow(gss_train_logit),
                                                          mindev = .001))))

# calculate each possible prune result for each fold
gss_cv_tree_prune <- expand.grid(gss_tree_cv$id, 2:30) %>%
  as_tibble() %>%
  mutate(Var2 = as.numeric(Var2)) %>%
  rename(id = Var1,
         k = Var2) %>%
  left_join(gss_tree_cv) %>%
  mutate(prune = map2(tree, k, ~ prune.tree(.x, best = .y)),
         estimate = map2(prune, splits, ~ predict(.x, newdata = assessment(.y))[,2]),
         truth = map(splits, ~ assessment(.x)$colrac)) %>%
  unnest(estimate, truth) %>%
  group_by(k) %>%
  accuracy(truth = truth, estimate = factor(round(estimate), levels = 0:1, labels = c("0", "1")))

#ggplot(gss_cv_tree_prune, aes(k, 1 - .estimate)) +
 # geom_point() +
#  geom_line() +
 # geom_vline(xintercept = gss_cv_tree_prune$k[[which.max(gss_cv_tree_prune$.estimate)]],
#             linetype = 2) +
#  labs(title = "Colrac predictions",
#       subtitle = "CV Decision Tree",
#       x = "Number of terminal nodes",
#       y = "10-fold CV error rate")


#find auc
tree_auc <- expand.grid(gss_tree_cv$id, 2:30) %>%
  as_tibble() %>%
  mutate(Var2 = as.numeric(Var2)) %>%
  rename(id = Var1,
         k = Var2) %>%
  left_join(gss_tree_cv) %>%
  mutate(prune = map2(tree, k, ~ prune.tree(.x, best = .y)),
         estimate = map2(prune, splits, ~ predict(.x, newdata = assessment(.y))[,2]),
         truth = map(splits, ~ assessment(.x)$colrac)) %>%
  unnest(estimate, truth) %>%
  mutate(estimate = round(estimate)) %>%
  group_by(k) %>%
  roc(response = truth, predictor = estimate)

```



###Bagging

```{r, warning=FALSE, message=FALSE}




gss_bag <- gss_train %>%
  mutate(colrac = make.names(as.character(if_else(colrac == 1, TRUE, FALSE))))


gss_bag_cv <- train(colrac ~ .,
                            data = gss_bag, 
                            method = "treebag",
                            metric = "ROC",
                            trControl = ctrl)



bag_error<- (gss_bag_cv$results$Sens+gss_bag_cv$results$Spec)/2

```



###Random Forest -- DO NOT USE
```{r, warning=FALSE, message=FALSE}

hyper_grid <- expand.grid(
  mtry = seq(14, 24, by = 2),
  node_size = seq(3, 9, by = 2),
  sampe_size = c(.55, .632, .70, .80),
  OOB_RMSE = 0)


# ranger: fast implementation of random forests (Breiman 2001)

for(i in 1:nrow(hyper_grid)) {
  # train model
  model <- ranger(  
    formula = colrac ~ ., 
    data = gss_train, 
    num.trees = 500,
    mtry = hyper_grid$mtry[i],
    min.node.size = hyper_grid$node_size[i],
    sample.fraction = hyper_grid$sampe_size[i],
    seed = 123)
  
  # add OOB error to grid
  hyper_grid$OOB_RMSE[i] <- sqrt(model$prediction.error)
}


# numerically inspect the model
hyper_grid <- hyper_grid %>% 
  arrange(OOB_RMSE)


best_rf <- ranger(
    formula = colrac ~ ., 
    data = gss_train_logit, 
    num.trees = 500,
    mtry = hyper_grid$mtry[[1]],
    min.node.size = hyper_grid$node_size[[1]],
    sample.fraction = hyper_grid$sampe_size[[1]])


rf_error <- best_rf$prediction.error

preds_num <- as.numeric(best_rf$predictions)

rf_auc <- roc(response = gss_train$colrac, predictor = preds_num)

```

try a diff package

```{r}
testgrid2 <- expand.grid(
  mtry = seq(14, 24, by = 3),
  min.node.size = seq(3, 9, by = 3),
  splitrule = "gini")


rf_cv <- train(colrac ~ ., 
                       data = gss_bag,
                       method = 'ranger',
                       metric = 'Accuracy',
                       tuneGrid = testgrid2,
                       trControl = ctrl)
```


###Boosting

```{r, warning=FALSE, message=FALSE, results='hide'}
gss_boost_cv <- train(colrac ~ .,
                            data = gss_bag, 
                            method = "gbm",
                            metric = "ROC",
                            trControl = ctrl)



boost_error<- (gss_boost_cv$results$Sens+gss_boost_cv$results$Spec)/2
```



##3. Evaluate the Models

Model performances:

  - Cross-validated error rate
  
      - Logistic regression: `r logit_error`
      
      - Naive Bayes: `r 1 - nb_error`
      
      - Elastic net regression: `r elastic_error`
      
      - Decision tree (CART): `r 1-gss_cv_tree_prune$.estimate[[which.max(gss_cv_tree_prune$.estimate)]]`
      
      - Bagging: `r 1-bag_error`
      
      - Random forest: `r rf_error`
      
      - Boosting: `r mean(1-boost_error)`
      
  - ROC/AUC
  
      - Logistic regression: `r logit_roc$auc`
      
      - Naive Bayes: `r nb_cv$results$ROC[which.max(nb_cv$results$ROC)]`
      
      - Elastic net regression: `r elastic_auc`
      
      - Decision tree (CART): `r tree_auc$auc`
      
      - Bagging: `r gss_bag_cv$results$ROC`
      
      - Random forest: `r rf_auc$auc`
      
      - Boosting: `r mean(gss_boost_cv$results$ROC)`
      
##4. Which is the best model? Defend your choice.

Considering the classification error rates and the AUC results, I believe the best model is the one produced by the elastic net regression. This model exhibits the lowest classification error rate at 16.02%, which is substantially better than the next best model (random forest, at 19.8% error). Further, its AUC performed second best, just behind the boosting model, at .864 compared to .875, respectively. Considering that the AUC results are somewhat close and the elastic net is clearly the best performer in terms of error rate, I argue that elastic net regression is the best modeling procedure.

##5. Evaluate the best model
```{r, warning=FALSE, message=FALSE}
#Generate Preds
test_preds <- as.numeric(predict(best_elastic_mod_bin, 
        newx = gss_test_x,
        type= "class"))

#Classification error rate
test_error <- sum(abs(test_preds-gss_test_y))/493

test_auc <- roc(response = gss_test_y, predictor = test_preds)

```

The test classification error rate is `r test_error` and the test AUC is `r test_auc$auc`. We can compare this to the training error rate of `r elastic_error` and training AUC of `r elastic_auc`. From these results, we can see that the model did not generalize as well as we would have hoped -- while we should expect the test error rate to be slightly lower than the cross validation error rate (because with CV, we are still (to a degree) validating against data that was used to build the model), an increase of about 5% in the error rate and drop of about .08 in the AUC are substantial steps backward. In further research, I would likely also consider testing the boosting model since it performed nearly as well on the test sample as elastic net.

