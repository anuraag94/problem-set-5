---
title: "Satinitigan_Karl_HW5"
author: "Karl Satinitigan"
date: "2/29/2020"
output: pdf_document
---

# MACS30100
# 
## Conceptual: Cost functions for classification trees

> The best to use when growing a decision tree is either the Gini index or the cross entropy. This is because they are more sensitive to node purity than the classification error rate, and they are quite similar numerically. The Gini index, though, is faster because it does not calculate the log.

> The best to use when pruning a decision tree is the classification error rate. This is because it has relatively better prediction accuracy for the final pruned tree.


## Application: Predicting attitudes towards racist college professors

```{r lm, echo = FALSE}

library(tidyverse)
library(tidymodels)
library(rcfss)
library(knitr)
library(splines)
library(lattice)
library(here)
library(patchwork)
library(margins)
library(ISLR)
library(boot)
library(readr)
library(glmnet)
library(caret)
library(pls)
library(pROC)
library(randomForest)
library(rpart)
library(rpart.plot)
library(ranger)
library(iml)
library(tree)
library(h2o)


set.seed(1234)
theme_set(theme_minimal())

gsstrain <- read_csv(url("https://raw.githubusercontent.com/ksatinitigan/problem-set-5/master/data/gss_train.csv"))

gsstest <- read_csv(url("https://raw.githubusercontent.com/ksatinitigan/problem-set-5/master/data/gss_test.csv"))

### Logistic regression

colrac_glm <- glm(colrac ~., family = binomial, data = gsstrain)
summary(colrac_glm)



```

```{r nb, echo = FALSE}

### Naive bayes

gssfeatures <- setdiff(names(gsstrain), "colrac")

gsstrainx <- gsstrain[, gssfeatures]

gsstrainy <- as.factor(gsstrain$colrac)

traincontrol_nb <- trainControl(method = "cv", number = 10)

searchgrid_nb <- expand.grid(usekernel = c(TRUE, FALSE), fL = 0:5, adjust = seq(0, 5, by = 1))

 
colrac_nb <- train(
  x = gsstrainx,
  y = gsstrainy,
  data = gsstrain,
  trControl = traincontrol_nb,
  method = "nb",
  tuneGrid = searchgrid_nb,
  preProc = c("zx", "center", "scale")
  )

 
colrac_nb$results %>% 
  top_n(5, wt = Accuracy) %>%
  arrange(desc(Accuracy))

plot(colrac_nb)

confusionMatrix(colrac_nb)

```



```{r en, echo = FALSE}

### Elastic net regression

gsstrainx <- model.matrix(colrac ~ ., gsstrain)[, -1]
gsstrainy <- gsstrain$colrac

gsstestx <- model.matrix(colrac ~., gsstest)[, -1]
gsstesty <- gsstest$colrac

for (i in seq(0, 1, .1))
colrac_enCV <- cv.glmnet(gsstrainx, gsstrainy, alpha=i)
bestlamelastic = colrac_enCV$lambda.min

colrac_en <- glmnet(gsstrainx, gsstrainy, alpha=1, lambda=bestlamelastic)
colrac_en$beta


```



```{r dt, echo = FALSE}

### Decision tree

gsstrain$colrac <- as.factor(gsstrain$colrac)

colrac_dt <- rpart(formula = colrac ~., data = gsstrain, method = "anova")
rpart.plot(colrac_dt)




```

```{r bag, echo = FALSE}

gsstrain$colrac <- as.factor(gsstrain$colrac)
colrac_bag <- train(colrac ~., data = gsstrain, method = "treebag", trControl = trainControl(method = "cv", number = 10), nbagg = 200, control = rpart.control(minsplit = 2, cp = 0))

colrac_bag


```

```{r rf, echo = FALSE}


### Random forest

n_features <- length(setdiff(names(gsstrain), "colrac"))

colrac_rf1 <- ranger(colrac ~., data = gsstrain, mtry = floor(n_features/3), respect.unordered.factors = "order", seed = 123)

default_rmse <- sqrt(colrac_rf1$prediction.error)
default_rmse

hypergrid_rf <- expand.grid(
  mtry = floor(n_features * c(.05, .15, .25, .333, .4)),
  min.node.size = c(1, 3, 5, 10), 
  replace = c(TRUE, FALSE),                               
  sample.fraction = c(.5, .63, .8),                       
  rmse = NA
)

for(i in seq_len(nrow(hypergrid_rf))) {
  fit <- ranger(
    formula         = colrac ~ ., 
    data            = gsstrain, 
    num.trees       = n_features * 10,
    mtry            = hypergrid_rf$mtry[i],
    min.node.size   = hypergrid_rf$min.node.size[i],
    replace         = hypergrid_rf$replace[i],
    sample.fraction = hypergrid_rf$sample.fraction[i],
    verbose         = FALSE,
    seed            = 123,
    respect.unordered.factors = 'order',
  )
  
  hypergrid_rf$rmse[i] <- sqrt(fit$prediction.error)
}


hypergrid_rf %>%
  arrange(rmse) %>%
  mutate(perc_gain = (default_rmse - rmse) / default_rmse * 100) %>%
  head(10)

h2o.no_progress()
h2o.init(max_mem_size = "5g")

train_h2o <- as.h2o(gsstrain)
response <- "colrac"
predictors <- setdiff(colnames(gsstrain), response)

h2o_rf1 <- h2o.randomForest(
    x = predictors, 
    y = response,
    training_frame = train_h2o, 
    ntrees = n_features * 10,
    seed = 123
)

h2o_rf1

hypergrid_rf <- list(
  mtries = floor(n_features * c(.05, .15, .25, .333, .4)),
  min_rows = c(1, 3, 5, 10),
  max_depth = c(10, 20, 30),
  sample_rate = c(.55, .632, .70, .80)
)

search_criteria <- list(
  strategy = "RandomDiscrete",
  stopping_metric = "mse",
  stopping_tolerance = 0.001,   
  stopping_rounds = 10,         
  max_runtime_secs = 60*5  
)

random_grid <- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_random_grid",
  x = predictors, 
  y = response, 
  training_frame = train_h2o,
  hyper_params = hypergrid_rf,
  ntrees = n_features * 10,
  seed = 123,
  stopping_metric = "RMSE",   
  stopping_rounds = 10,           
  stopping_tolerance = 0.005,     
  search_criteria = search_criteria
)

random_grid_perf <- h2o.getGrid(
  grid_id = "rf_random_grid", 
  sort_by = "mse", 
  decreasing = FALSE
)
random_grid_perf


```

```{r boost, echo = FALSE}

### Boosting

colrac_boost <- train(colrac ~ .,
                          data = gsstrain,
                          method = "gbm",
                          trControl = trainControl(method = "cv", 
                                                   number = 10, 
                                                   verboseIter = FALSE),
                          verbose = 0) 


colrac_boost

colrac_boost$results

```

```{r, echo = FALSE}

### Evaluating the models

confusionMatrix(colrac_glm)

confusionMatrix(colrac_nb)

confusionMatrix(colrac_en)

confusionMatrix(colrac_dt)

confusionMatrix(colrac_bag)

confusionMatrix(colrac_rf1)

confusionMatrix(colrac_boost)



```

```{r echo = FALSE}

### Evaluating the best models

```