---
title: "Problem Set 5"
author: "Pete Cuppernull"
date: "2/26/2020"
output: pdf_document
---

##Load Packages and Data
```{r setup, include=FALSE}
library(tidyverse)
library(tidymodels)
library(rsample)
library(glmnet)
library(leaps)
library(rcfss)
library(patchwork)
library(caret)
library(h2o)
library(ipred)
library(adabag)
library(randomForest)
library(ranger)
library(gbm)
library(tree)
library(pROC)

gss_test <- na.omit(read_csv("data/gss_test.csv"))
gss_train <- na.omit(read_csv("data/gss_train.csv"))

```


#Application

##2. Estimate the Models

###Logit
```{r}
gss_train_logit <- as_tibble(gss_train) %>%
  mutate(colrac = factor(colrac))

# function to generate assessment statistics for titanic model
holdout_results <- function(splits) {
  # Fit the model to the training set
  mod <- glm(colrac ~ ., data = analysis(splits),
             family = binomial)
  
  # `augment` will save the predictions with the holdout data set
  res <- augment(mod, newdata = assessment(splits)) %>% 
    as_tibble() %>%
    mutate(.prob = logit2prob(.fitted),
           .pred = round(.prob))

  # Return the assessment data set with the additional columns
  res
}

gss_logit_cv10 <- vfold_cv(data = gss_train_logit, v = 10) %>%
  mutate(results = map(splits, holdout_results)) %>%
  unnest(results) %>%
  mutate(.pred = factor(.pred)) %>%
  group_by(id) %>%
  accuracy(truth = colrac, estimate = .pred)

logit_error <- 1 - mean(gss_logit_cv10$.estimate, na.rm = TRUE)


##roc
logit_roc <- vfold_cv(data = gss_train_logit, v = 10) %>%
  mutate(results = map(splits, holdout_results)) %>%
  unnest(results) %>%
  group_by(id) %>%
  roc(response = colrac, predictor = .pred)


glimpse(logit_roc)

logit_roc$auc

```

###Naive Bayes
```{r}
h2o.no_progress()
h2o.init()

#Save DFs as h20 objects
train.h2o <- as.h2o(gss_train %>%
                      mutate(colrac = as.factor(colrac)))
test.h2o <- as.h2o(gss_test %>%
                      mutate(colrac = as.factor(colrac)))

#set variable names
y <- "colrac"
x <- setdiff(names(gss_train), y) 

#NB Model
nb <- h2o.naiveBayes(
  x = x, 
  y = y,
  training_frame = train.h2o,
  validation_frame = test.h2o,
  seed = 123,
  nfolds = 10)

nb_auc <- h2o.auc(nb, valid = TRUE)


nb
```

###Elastic net regression
```{r}
gss_train_x <- model.matrix(colrac ~ ., gss_train)[, -1]
gss_train_y <- gss_train$colrac

gss_test_x <- model.matrix(colrac ~ ., gss_test)[, -1]
gss_test_y <- gss_test$colrac

lasso    <- glmnet(gss_train_x, gss_train_y, alpha = 1.0) 
elastic1 <- glmnet(gss_train_x, gss_train_y, alpha = 0.25) 
elastic2 <- glmnet(gss_train_x, gss_train_y, alpha = 0.75) 
ridge    <- glmnet(gss_train_x, gss_train_y, alpha = 0.0)

fold_id <- sample(1:10, size = length(gss_train_y), replace = TRUE)

tuning_grid <- tibble::tibble(
  alpha      = seq(0, 1, by = .1),
  mse_min    = NA,
  mse_1se    = NA,
  lambda_min = NA,
  lambda_1se = NA
)

for(i in seq_along(tuning_grid$alpha)) {
  # fit CV model for each alpha value
  fit <- cv.glmnet(gss_train_x, 
                   gss_train_y, 
                   alpha = tuning_grid$alpha[i], 
                   foldid = fold_id)
  
  # extract MSE and lambda values
  tuning_grid$mse_min[i]    <- fit$cvm[fit$lambda == fit$lambda.min]
  tuning_grid$mse_1se[i]    <- fit$cvm[fit$lambda == fit$lambda.1se]
  tuning_grid$lambda_min[i] <- fit$lambda.min
  tuning_grid$lambda_1se[i] <- fit$lambda.1se
}


elastic_mse <- min(tuning_grid$mse_min)

tuning_grid

##now that we have the model, lets generate predictions so we can calculate error rate and AUC
best_elastic_mod <- cv.glmnet(gss_train_x, 
                   gss_train_y, 
                   alpha = 0.3, 
                   foldid = fold_id)


roc(response = colrac, predictor = .pred)

glimpse(best_elastic_mod)

elastic_error <- mean(best_elastic_mod$cvm)

best_elastic_mod_auc <- cv.glmnet(gss_train_x, 
                   gss_train_y, 
                   alpha = 0.3, 
                   family = "binomial",
                   type.measure = "auc",
                   foldid = fold_id)


elastic_auc <- mean(best_elastic_mod_auc$cvm)
```

###Decision tree (CART)
```{r}
# generate 10-fold CV trees
gss_tree_cv <- gss_train_logit %>%
  vfold_cv(v = 10) %>%
  mutate(tree = map(splits, ~ tree(colrac ~ ., data = analysis(.x),
                                   control = tree.control(nobs = nrow(gss_train_logit),
                                                          mindev = .001))))

# calculate each possible prune result for each fold
gss_cv_tree_prune <- expand.grid(gss_tree_cv$id, 2:30) %>%
  as_tibble() %>%
  mutate(Var2 = as.numeric(Var2)) %>%
  rename(id = Var1,
         k = Var2) %>%
  left_join(gss_tree_cv) %>%
  mutate(prune = map2(tree, k, ~ prune.tree(.x, best = .y)),
         estimate = map2(prune, splits, ~ predict(.x, newdata = assessment(.y))[,2]),
         truth = map(splits, ~ assessment(.x)$colrac)) %>%
  unnest(estimate, truth) %>%
  group_by(k) %>%
  accuracy(truth = truth, estimate = factor(round(estimate), levels = 0:1, labels = c("0", "1")))

ggplot(gss_cv_tree_prune, aes(k, 1 - .estimate)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept = gss_cv_tree_prune$k[[which.max(gss_cv_tree_prune$.estimate)]],
             linetype = 2) +
  labs(title = "Colrac predictions",
       subtitle = "CV Decision Tree",
       x = "Number of terminal nodes",
       y = "10-fold CV error rate")

gss_cv_tree_prune$k[[which.max(gss_cv_tree_prune$.estimate)]]

gss_cv_tree_prune$.estimate[[which.max(gss_cv_tree_prune$.estimate)]]

#find auc
tree_auc <- expand.grid(gss_tree_cv$id, 2:30) %>%
  as_tibble() %>%
  mutate(Var2 = as.numeric(Var2)) %>%
  rename(id = Var1,
         k = Var2) %>%
  left_join(gss_tree_cv) %>%
  mutate(prune = map2(tree, k, ~ prune.tree(.x, best = .y)),
         estimate = map2(prune, splits, ~ predict(.x, newdata = assessment(.y))[,2]),
         truth = map(splits, ~ assessment(.x)$colrac)) %>%
  unnest(estimate, truth) %>%
  mutate(estimate = round(estimate)) %>%
  group_by(k) %>%
  roc(response = truth, predictor = estimate)

```



###Bagging

```{r}
ctrl <- trainControl(method = "cv",     # Cross-validation
                     number = 10,      # 10 folds
                     classProbs = TRUE,                  # For AUC
                     summaryFunction = twoClassSummary)  # For AUC

# Cross validate the credit model using "treebag" method; 
# Track AUC (Area under the ROC curve)
set.seed(1)  # for reproducibility


gss_bag <- gss_train %>%
  mutate(colrac = make.names(as.character(if_else(colrac == 1, TRUE, FALSE))))


gss_bag_cv <- train(colrac ~ .,
                            data = gss_bag, 
                            method = "treebag",
                            metric = "ROC",
                            trControl = ctrl)

gss_bag_cv$results$ROC



glimpse(gss_bag_cv)

bag_error<- (gss_bag_cv$results$Sens+gss_bag_cv$results$Spec)/2

```



###Random Forest -- I NEED TO MODIFY THIS SO IT IS CV
```{r}
default_rf <- randomForest(
  formula = colrac ~ .,
  data    = gss_train
)


hyper_grid <- expand.grid(
  mtry = seq(14, 24, by = 2),
  node_size = seq(3, 9, by = 2),
  sampe_size = c(.55, .632, .70, .80),
  OOB_RMSE = 0
)

# total number of combinations
nrow(hyper_grid)

# ranger: fast implementation of random forests (Breiman 2001)

for(i in 1:nrow(hyper_grid)) {
  # train model
  model <- ranger(  
    formula = colrac ~ ., 
    data = gss_train, 
    num.trees = 500,
    mtry = hyper_grid$mtry[i],
    min.node.size = hyper_grid$node_size[i],
    sample.fraction = hyper_grid$sampe_size[i],
    seed = 123
  )
  
  # add OOB error to grid
  hyper_grid$OOB_RMSE[i] <- sqrt(model$prediction.error)
}


# numerically inspect the model
hyper_grid <- hyper_grid %>% 
  arrange(OOB_RMSE)
hyper_grid %>%
  head(10)

best_rf <- ranger(
    formula = colrac ~ ., 
    data = gss_train_logit, 
    num.trees = 500,
    mtry = hyper_grid$mtry[[1]],
    min.node.size = hyper_grid$node_size[[1]],
    sample.fraction = hyper_grid$sampe_size[[1]]
  )


glimpse(best_rf)

rf_error <- best_rf$prediction.error

preds_num <- as.numeric(best_rf$predictions)

rf_auc <- roc(response = gss_train$colrac, predictor = preds_num)
```

###Boosting
```{r}
boost_gss <- train(colrac ~ .,
                          data = gss_train_logit,
                          method = "gbm",
                          trControl = trainControl(method = "cv", 
                                                   number = 10, 
                                                   verboseIter = FALSE),
                          verbose = 0) # fit silently

lambda <- seq(0.0001, 0.04, by = 0.001)

boosted_tree_train <- function(lambda){
  model <- gbm(colrac ~ .,
    data = gss_train_logit,
    distribution="gaussian",
    n.trees=1000,
    shrinkage = lambda,
    cv.folds = 10)
  
mean(model$train.error)
}

  modeltest2 <- gbm(colrac ~ .,
    data = gss_train,
    distribution= "bernoulli",
    n.trees=1000,
    shrinkage = .01,
    cv.folds = 10)
  
mean(modeltest$train.error)

modeltest2


boost_train_error <- map_dbl(lambda, boosted_tree_train)

boosted_errors <- as.data.frame(cbind(lambda, boost_train_error)) %>%
  rename(mse = `boost_train_mses`)

which.min(boosted_errors$boost_train_error)

glimpse(modeltest2)
modeltest2$
modeltest$self.statistics$discrimination

```

lets try with caret
```{r}
objModel <- train(trainDF[,predictorsNames], trainDF[,outcomeName], 
                  method='gbm', 
                  trControl=objControl,  
                  metric = "ROC",
                  preProc = c("center", "scale"))

gss_boost_cv <- train(colrac ~ .,
                            data = gss_bag, 
                            method = "gbm",
                            metric = "ROC",
                            trControl = ctrl)

mean(gss_boost_cv$results$ROC)



glimpse(gss_boost_cv)

boost_error<- (gss_boost_cv$results$Sens+gss_boost_cv$results$Spec)/2
```



##3. Evaluate the Models

Model performances:
  - Cross-validated error rate
      - Logistic regression: `r logit_error`
      - Naive Bayes: 0.253388 (need to figure out how to do this automatically)
      - Elastic net regression: `r elastic_error`
      - Decision tree (CART): `r gss_cv_tree_prune$.estimate[[which.max(gss_cv_tree_prune$.estimate)]]`
      - Bagging: `r 1-bag_error`
      - Random forest: `r rf_error`
      - Boosting: `r 1-boost_error`
  - ROC/AUC
      - Logistic regression: `r logit_roc$auc`
      - Naive Bayes: `r nb_auc`
      - Elastic net regression: `r elastic_auc`
      - Decision tree (CART): `r tree_auc$auc`
      - Bagging: `r gss_bag_cv$results$ROC`
      - Random forest: `r rf_auc`
      - Boosting: `r mean(gss_boost_cv$results$ROC)` not sure if this is correct
